{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import langid\n",
    "import re\n",
    "import pickle\n",
    "import config\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords  # for using english stopwords\n",
    "from gensim.models.phrases import Phrases\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tweetf0rm.handler.oracle_handler import OracleHandler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.utils import deaccent, decode_htmlentities, lemmatize\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(my_tags))\n",
    "    target_names = my_tags\n",
    "    plt.xticks(tick_marks, target_names, rotation=90)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
    "    print('accuracy %s' % accuracy_score(target, predictions))\n",
    "    cm = confusion_matrix(target, predictions)\n",
    "    print('confusion matrix\\n %s' % cm)\n",
    "    print('(row=expected, col=predicted)')\n",
    "    \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot_confusion_matrix(cm_normalized, title + ' Normalized')\n",
    "    \n",
    "def most_influential_words(clf, vectorizer, category_index=0, num_words=10):\n",
    "    features = vectorizer.get_feature_names()\n",
    "    max_coef = sorted(enumerate(clf.coef_[category_index]), key=lambda x:x[1], reverse=True)\n",
    "    return [features[x[0]] for x in max_coef[:num_words]]    \n",
    "\n",
    "def remove_url(documents):\n",
    "    return [(doc[0],re.sub(r\"(?:\\@|http?\\://)\\S+\", \"\", doc[1])) for doc in documents]\n",
    "\n",
    "# def filter_lang(lang, documents):\n",
    "#     return (doc for doc in documents if langid.classify(doc[1])[0] == lang)\n",
    "\n",
    "def filter_lang(text,lang='en'):\n",
    "    return  langid.classify(text)[0] == lang\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \"<hashtag> {} <allcaps>\".format(hashtag_body)\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r\"(?:\\@|https|http?\\://)\\S+\", \"\", tweet) # remove urls\n",
    "    tweet = \" \".join([i for i in tweet.lower().split() if i not in stops]) #tokenize and remove stop words\n",
    "    tweet = ''.join(ch for ch in tweet if ch not in exclude)# remove more words\n",
    "    tweet = \" \".join(lemma.lemmatize(word) for word in tweet.split())\n",
    "    return tweet\n",
    "\n",
    "def get_similar_word_correlation(train_texts)\n",
    "    dictionay = Dictionary(tweet_processed)\n",
    "\n",
    "    correlation_matrix = scipy.sparse.identity(len(vocab), format=\"dok\")\n",
    "    for tokens in dictionay.values():\n",
    "        similar_words = []\n",
    "        try:\n",
    "            similar_words = [x[0] for x in w2vmodel.most_similar(tokens.lower(), topn=5) if x[1] > 0.5]\n",
    "        except:\n",
    "    #         raise\n",
    "            pass\n",
    "        for similar_word in similar_words:\n",
    "            if similar_word in vocab:\n",
    "                correlation_matrix[dictionay.token2id[word], dictionay.token2id[similar_word]] = 1\n",
    "\n",
    "#         term_frequency_vector += term_frequency_vector * correlation_matrix\n",
    "\n",
    "# def allcaps(text):\n",
    "#     text = text.group()\n",
    "#     return text.lower() + \" <allcaps> \"\n",
    "\n",
    "\n",
    "# def preprocess_tweet2(text):\n",
    "#     # Different regex parts for smiley faces\n",
    "#     eyes = r\"[8:=;]\"\n",
    "#     nose = r\"['`\\-]?\"\n",
    "\n",
    "#     # function so code less repetitive\n",
    "#     def re_sub(pattern, repl):\n",
    "#         return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "#     text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "#     text = re_sub(r\"/\",\" / \")\n",
    "#     text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "#     text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "#     text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "#     text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "#     text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "#     text = re_sub(r\"<3\",\"<heart>\")\n",
    "#     text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" <number> \")\n",
    "#     text = re_sub(r\"#\\S+\", hashtag)\n",
    "#     text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "#     text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "\n",
    "#     ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "#     # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "#     text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "    \n",
    "#     text = ''.join(ch for ch in text if ch not in exclude)  # Remove punctuation\n",
    "\n",
    "#     return text.lower()\n",
    "\n",
    "def preprocess_text(tweet):\n",
    "    \"\"\"\n",
    "    Function to process an aggregated user profile. This does the following:\n",
    "    1. Decode html entities. eg. \"AT&amp;T\" will become \"AT&T\"\n",
    "    2. Deaccent\n",
    "    3. Remove links.\n",
    "    4. Remove any user mentions (@name).\n",
    "    5. Lemmatize and remove stopwords.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : String. If train_texts is a list of tweets, ' '.join and pass\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    text : preprocessed (tokenized) tweet.\n",
    "    \"\"\"\n",
    "    tweet = re.sub(r\"(?:\\@|https|http?\\://)\\S+\", \"\", tweet) # remove urls\n",
    "    tweet = \" \".join([i for i in tweet.lower().split() if i not in stops]) #tokenize and remove stop words\n",
    "    tweet = ''.join(ch for ch in tweet if ch not in exclude)# remove more words\n",
    "    \n",
    "    tweet = decode_htmlentities(tweet)\n",
    "    tweet = deaccent(tweet)\n",
    "    tweet = tweet.encode('ascii', 'ignore')  # To prevent UnicodeDecodeErrors later on\n",
    "#     tweet = re.sub(r'http\\S+', '', str(tweet))  # Step 3\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)  # Step 4\n",
    "    tweet = tweet.split()\n",
    "    tweet = lemmatize(' '.join(tweet), re.compile('(NN)'), stopwords=stops, min_length=3, max_length=15)\n",
    "    tweet = [word.split('/')[0] for word in tweet]\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def get_dataframes(pycon_dict):\n",
    "    \"\"\"\n",
    "    Function to get train and test dataframes (without any preprocessing).\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    pycon_dict: The twitter user dictionary being used.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    train, test: Train and test dataframes.\n",
    "    \"\"\"\n",
    "    train = pd.DataFrame(columns=columns)\n",
    "    test = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for category in pycon_dict:\n",
    "        for entity in pycon_dict[category]:\n",
    "            train_texts = []\n",
    "            test_texts = []\n",
    "            num_texts = len(pycon_dict[category][entity])  # To get number of tweets\n",
    "            train_indices = np.random.choice(num_texts, int(0.9 * num_texts), replace=False)  # Random selection\n",
    "            test_indices = [i for i in range(num_texts) if i not in train_indices]  # Rest go into test set\n",
    "            train_texts.extend(pycon_dict[category][entity][i].text for i in train_indices)  # Add to train texts\n",
    "            test_texts.extend(pycon_dict[category][entity][i].text for i in test_indices)  # Add to test texts\n",
    "            #### Create train dataframe ####\n",
    "            train_texts = ' '.join(train_texts)\n",
    "            df_train = pd.DataFrame([[train_texts, categories_map[category], category]], columns=columns)\n",
    "            train = train.append(df_train, ignore_index=True)\n",
    "            #### Create test dataframe ####\n",
    "            test_texts = ' '.join(test_texts)\n",
    "            df_test = pd.DataFrame([[test_texts, categories_map[category], category]], columns=columns)\n",
    "            test = test.append(df_test, ignore_index=True)\n",
    "            \n",
    "    return train, test\n",
    "\n",
    "def predicit_categroies(df, bigram, clf):\n",
    "    predict_texts = [bigram[message] for message in  df['processed_text']]\n",
    "    predict_texts_features = count_vectorizer.transform(' '.join(text) for text in predict_texts)\n",
    "    return clf.predict(predict_texts_features)\n",
    "\n",
    "db = OracleHandler()\n",
    "db_eng = create_engine(config.SQLALCHEMY_DATABASE_URI, encoding='utf8')\n",
    "stops = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "columns = ['message', 'category_id', 'category']\n",
    "categories_map = {0: u'Business & CEOs',\n",
    "                  1: u'Music',\n",
    "                  2: u'Entertainment',\n",
    "                  3: u'Fashion, Travel & Lifestyle',\n",
    "                  4: u'Sports',\n",
    "                  5: u'Tech',\n",
    "                  6: u'Politics',\n",
    "                  7: u'Science',\n",
    "                  u'Business & CEOs': 0,\n",
    "                  u'Entertainment': 2,\n",
    "                  u'Fashion, Travel & Lifestyle': 3,\n",
    "                  u'Music': 1,\n",
    "                  u'Politics': 6,\n",
    "                  u'Science': 7,\n",
    "                  u'Sports': 4,\n",
    "                  u'Tech': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the trainging data \n",
    "pycon_dict = pickle.load(open(\"pycon_dict.pkl\", \"rb\"))\n",
    "## or ###\n",
    "def getTweets(category_dict, category): \n",
    "    \"\"\" Function to get the tweets for each handle in the dictionary in the particular category. Parameters: ---------- category_dict: User category dictionary consisting of categories and user handles. category: String. Name of the category. Returns: ------- category_dict: Dictionary with the most recent 200 tweets of all user handles. \"\"\" \n",
    "    for handle in category_dict[category]: \n",
    "        category_dict[category][handle] = api.GetUserTimeline(screen_name=handle, count=200) \n",
    "        return category_dict\n",
    "\n",
    "\n",
    "my_tags = pycon_dict.keys()\n",
    "train, test = get_dataframes(pycon_dict)\n",
    "train_texts = train['message'].apply(preprocess_text)\n",
    "train_categories = train['category_id'].astype(int)\n",
    "bigram = Phrases(train_texts)# For collocation detection\n",
    "train_texts = [bigram[profile] for profile in train_texts]\n",
    "\n",
    "#for the test data\n",
    "test_texts = test['message'].apply(preprocess_text)\n",
    "test_texts = [bigram[message] for message in test_texts]\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=5000)\n",
    "train_count_features = count_vectorizer.fit_transform(' '.join(text) for text in train_texts)\n",
    "# similar_word_correlation_matrix =  get_similar_word_correlation(train_texts)\n",
    "# train_count_features = train_count_features * similar_word_correlation_matrix\n",
    "clf_model = LogisticRegression()\n",
    "clf_model = clf_model.fit(train_count_features, train_categories)\n",
    "test_count_features = count_vectorizer.transform(' '.join(text) for text in test_texts)\n",
    "predictions = clf_model.predict(test_count_features)\n",
    "%matplotlib inline\n",
    "# evaluate_prediction(predictions, test['category_id'])\n",
    "# train_count_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "        select user_handle, text\n",
    "        from tweets_timeline\n",
    "        where lang in ('en', 'und') \n",
    "        --and user_handle = 'osayamenomigie'\n",
    "        and user_handle in  ('osayamenomigie','Princeolaoluwa', 'Focusj3')\n",
    "        \"\"\"\n",
    "df = pd.read_sql_query(sql, db_eng)\n",
    "df2 = df.copy()\n",
    "df2 = df2[df2.apply(lambda row: filter_lang(row['text']), axis=1)] #filter for english\n",
    "# df2 = df2.groupby(['user_handle'])['text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "# df2['text'] = df2['text'].map(clean_tweet)# remove urls\n",
    "df2['processed_text'] = df2['text'].apply(preprocess_text)\n",
    "final_predictions = predicit_categroies(df2, bigram, clf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_handle</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Princeolaoluwa</td>\n",
       "      <td>@_oribz @Healthertainmet just keep praying, pr...</td>\n",
       "      <td>[prayer, lot]</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Princeolaoluwa</td>\n",
       "      <td>@honorable creations, i give your fabric a tou...</td>\n",
       "      <td>[creation, fabric, touch, life, hotline]</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Princeolaoluwa</td>\n",
       "      <td>https://t.co/8OlpAK6DTv https://t.co/8OlpAK6DT...</td>\n",
       "      <td>[facebook]</td>\n",
       "      <td>Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Princeolaoluwa</td>\n",
       "      <td>Myself and Love https://t.co/p1u75AAn2X</td>\n",
       "      <td>[]</td>\n",
       "      <td>Fashion, Travel &amp; Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Princeolaoluwa</td>\n",
       "      <td>#ITS NOT TOO LATE# 9JA FOR LIFE</td>\n",
       "      <td>[life]</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Princeolaoluwa</td>\n",
       "      <td>Check out Camera + - Selfies + in BlackBerry W...</td>\n",
       "      <td>[check, camera, blackberry, world]</td>\n",
       "      <td>Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Princeolaoluwa</td>\n",
       "      <td>Visit http://t.co/GrCXzAwBq1 to register for a...</td>\n",
       "      <td>[visit, register, security, card, today, treat...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Princeolaoluwa</td>\n",
       "      <td>Dunno what has gotten into me.. I'm loving it!!</td>\n",
       "      <td>[]</td>\n",
       "      <td>Fashion, Travel &amp; Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Princeolaoluwa</td>\n",
       "      <td>C004B2F6B its just getting better!</td>\n",
       "      <td>[]</td>\n",
       "      <td>Fashion, Travel &amp; Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Princeolaoluwa</td>\n",
       "      <td>Finally!!!! http://t.co/XYpzwGhCOU</td>\n",
       "      <td>[]</td>\n",
       "      <td>Fashion, Travel &amp; Lifestyle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_handle                                               text                                     processed_text                   prediction\n",
       "0   Princeolaoluwa  @_oribz @Healthertainmet just keep praying, pr...                                      [prayer, lot]                Entertainment\n",
       "1   Princeolaoluwa  @honorable creations, i give your fabric a tou...           [creation, fabric, touch, life, hotline]                      Science\n",
       "2   Princeolaoluwa  https://t.co/8OlpAK6DTv https://t.co/8OlpAK6DT...                                         [facebook]                         Tech\n",
       "3   Princeolaoluwa            Myself and Love https://t.co/p1u75AAn2X                                                 []  Fashion, Travel & Lifestyle\n",
       "4   Princeolaoluwa                    #ITS NOT TOO LATE# 9JA FOR LIFE                                             [life]                      Science\n",
       "5   Princeolaoluwa  Check out Camera + - Selfies + in BlackBerry W...                 [check, camera, blackberry, world]                         Tech\n",
       "6   Princeolaoluwa  Visit http://t.co/GrCXzAwBq1 to register for a...  [visit, register, security, card, today, treat...                Entertainment\n",
       "7   Princeolaoluwa    Dunno what has gotten into me.. I'm loving it!!                                                 []  Fashion, Travel & Lifestyle\n",
       "10  Princeolaoluwa                 C004B2F6B its just getting better!                                                 []  Fashion, Travel & Lifestyle\n",
       "11  Princeolaoluwa                 Finally!!!! http://t.co/XYpzwGhCOU                                                 []  Fashion, Travel & Lifestyle"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['prediction'] = [categories_map[i] for i in final_predictions]\n",
    "# df2.groupby(['user_handle', 'prediction']).agg(['count'])\n",
    "# df2.groupby(['user_handle', 'prediction']).count()\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.width', 1000)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_handle</th>\n",
       "      <th>prediction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">Focusj3</th>\n",
       "      <th>Business &amp; CEOs</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entertainment</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fashion, Travel &amp; Lifestyle</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Music</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politics</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tech</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">Princeolaoluwa</th>\n",
       "      <th>Business &amp; CEOs</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entertainment</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fashion, Travel &amp; Lifestyle</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Music</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politics</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tech</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">osayamenomigie</th>\n",
       "      <th>Business &amp; CEOs</th>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entertainment</th>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fashion, Travel &amp; Lifestyle</th>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Music</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politics</th>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports</th>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tech</th>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text  processed_text\n",
       "user_handle    prediction                                       \n",
       "Focusj3        Business & CEOs                 5               5\n",
       "               Entertainment                   3               3\n",
       "               Fashion, Travel & Lifestyle    17              17\n",
       "               Music                           6               6\n",
       "               Politics                        1               1\n",
       "               Science                         3               3\n",
       "               Sports                          1               1\n",
       "               Tech                            3               3\n",
       "Princeolaoluwa Business & CEOs                 5               5\n",
       "               Entertainment                  13              13\n",
       "               Fashion, Travel & Lifestyle    20              20\n",
       "               Music                           1               1\n",
       "               Politics                        1               1\n",
       "               Science                         5               5\n",
       "               Sports                          2               2\n",
       "               Tech                            6               6\n",
       "osayamenomigie Business & CEOs               177             177\n",
       "               Entertainment                 102             102\n",
       "               Fashion, Travel & Lifestyle   228             228\n",
       "               Music                          43              43\n",
       "               Politics                       54              54\n",
       "               Science                        63              63\n",
       "               Sports                         44              44\n",
       "               Tech                          153             153"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby(['user_handle', 'prediction']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most_influential_words(clf_model, count_vectorizer, category_index=2, num_words=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "File F:/NYC/NYU/SM/3/SNLP/Project/Data/train.tsv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a47721ef2635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Prepare the Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# ----------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F:/NYC/NYU/SM/3/SNLP/Project/Data/train.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Read Data\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File F:/NYC/NYU/SM/3/SNLP/Project/Data/train.tsv does not exist"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as p\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pylab as pl\n",
    "from sklearn import linear_model, cross_validation, metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn import preprocessing\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "modelType = \"notext\"\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Prepare the Data\n",
    "# ----------------------------------------------------------\n",
    "training_data = np.array(p.read_table('F:/NYC/NYU/SM/3/SNLP/Project/Data/train.tsv'))\n",
    "print (\"Read Data\\n\")\n",
    "\n",
    "# get the target variable and set it as Y so we can predict it\n",
    "Y = training_data[:,-1]\n",
    "\n",
    "print(Y)\n",
    "\n",
    "# not all data is numerical, so we'll have to convert those fields\n",
    "# fix \"is_news\":\n",
    "training_data[:,17] = [0 if x == \"?\" else 1 for x in training_data[:,17]]\n",
    "\n",
    "# fix -1 entries in hasDomainLink\n",
    "training_data[:,14] = [0 if x ==\"-1\" else x for x in training_data[:,10]]\n",
    "\n",
    "# fix \"news_front_page\":\n",
    "training_data[:,20] = [999 if x == \"?\" else x for x in training_data[:,20]]\n",
    "training_data[:,20] = [1 if x == \"1\" else x for x in training_data[:,20]]\n",
    "training_data[:,20] = [0 if x == \"0\" else x for x in training_data[:,20]]\n",
    "\n",
    "# fix \"alchemy category\":\n",
    "training_data[:,3] = [0 if x==\"arts_entertainment\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [1 if x==\"business\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [2 if x==\"computer_internet\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [3 if x==\"culture_politics\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [4 if x==\"gaming\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [5 if x==\"health\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [6 if x==\"law_crime\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [7 if x==\"recreation\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [8 if x==\"religion\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [9 if x==\"science_technology\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [10 if x==\"sports\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [11 if x==\"unknown\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [12 if x==\"weather\" else x for x in training_data[:,3]]\n",
    "training_data[:,3] = [999 if x==\"?\" else x for x in training_data[:,3]]\n",
    "\n",
    "print (\"Corrected outliers data\\n\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Models\n",
    "# ----------------------------------------------------------\n",
    "if modelType == \"notext\":\n",
    "    print (\"no text model\\n\")\n",
    "    #ignore features which are useless\n",
    "    X = training_data[:,list([3, 5, 6, 7, 8, 9, 10, 14, 15, 16, 17, 19, 20, 22, 25])]\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    print(\"initialized scaler \\n\")\n",
    "    scaler.fit(X,Y)\n",
    "    print(\"fitted train data and labels\\n\")\n",
    "    X = scaler.transform(X)\n",
    "    print(\"Transformed train data\\n\")\n",
    "    svc = SVC(kernel = \"linear\")\n",
    "    print(\"Initialized SVM\\n\")\n",
    "    rfecv = RFECV(estimator = svc, cv = 5, loss_func = zero_one_loss, verbose = 1)\n",
    "    print(\"Initialized RFECV\\n\")\n",
    "    rfecv.fit(X,Y)\n",
    "    print(\"Fitted train data and label\\n\")\n",
    "    rfecv.support_\n",
    "    print (\"Optimal Number of features : %d\" % rfecv.n_features_)\n",
    "    savetxt('rfecv.csv', rfecv.ranking_, delimiter=',', fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
